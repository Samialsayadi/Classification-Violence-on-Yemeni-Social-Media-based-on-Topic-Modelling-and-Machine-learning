{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense, GRU, Embedding,LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert text to word embedding (Using Glove Vector 50 Dim):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(\"glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violence Classification based RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model_RCNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50):\n",
    "\n",
    "    kernel_size = 2\n",
    "    filters = 256\n",
    "    pool_size = 2\n",
    "    gru_node = 256\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    \n",
    "    model.add(LSTM(gru_node, recurrent_dropout=0.2))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(nclasses))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus = pd.read_csv(r\"aji-Arabic_corpus.csv\")\n",
    "\n",
    "import nltk \n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "Corpus = pd.read_csv(r\"Violence in Yemens Social Media.csv\",sep=';')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_term=[]\n",
    "for term in Corpus.Text:\n",
    "    sent = \" \".join(w for w in nltk.wordpunct_tokenize(term) if w.lower() in words or not w.isalpha())\n",
    "    list_term.append(sent)\n",
    "dic={'Text':list_term, 'Dominant_Topic':Corpus.Dominant_Topic}\n",
    "Corpus=pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(Corpus['Text'],Corpus['Dominant_Topic'],test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7163 unique tokens.\n",
      "(77062, 500)\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 50)           358200    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500, 50)           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 499, 256)          25856     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 249, 256)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 248, 256)          131328    \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 124, 256)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 123, 256)          131328    \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 61, 256)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 60, 256)           131328    \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 30, 256)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,912,829\n",
      "Trainable params: 2,912,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "482/482 - 1117s - loss: 1.1989 - accuracy: 0.5765 - val_loss: 0.7963 - val_accuracy: 0.7205 - 1117s/epoch - 2s/step\n",
      "Epoch 2/15\n",
      "482/482 - 1318s - loss: 0.6859 - accuracy: 0.7626 - val_loss: 0.5198 - val_accuracy: 0.8404 - 1318s/epoch - 3s/step\n",
      "Epoch 3/15\n",
      "482/482 - 1307s - loss: 0.4779 - accuracy: 0.8449 - val_loss: 0.3900 - val_accuracy: 0.8715 - 1307s/epoch - 3s/step\n",
      "Epoch 4/15\n",
      "482/482 - 1440s - loss: 0.3891 - accuracy: 0.8719 - val_loss: 0.3456 - val_accuracy: 0.8860 - 1440s/epoch - 3s/step\n",
      "Epoch 5/15\n",
      "482/482 - 1274s - loss: 0.3460 - accuracy: 0.8859 - val_loss: 0.3325 - val_accuracy: 0.8913 - 1274s/epoch - 3s/step\n",
      "Epoch 6/15\n",
      "482/482 - 2421s - loss: 0.3245 - accuracy: 0.8921 - val_loss: 0.3201 - val_accuracy: 0.8976 - 2421s/epoch - 5s/step\n",
      "Epoch 7/15\n",
      "482/482 - 1212s - loss: 0.3035 - accuracy: 0.8993 - val_loss: 0.3075 - val_accuracy: 0.8989 - 1212s/epoch - 3s/step\n",
      "Epoch 8/15\n",
      "482/482 - 1869s - loss: 0.2907 - accuracy: 0.9040 - val_loss: 0.3044 - val_accuracy: 0.9011 - 1869s/epoch - 4s/step\n",
      "Epoch 9/15\n",
      "482/482 - 1892s - loss: 0.2760 - accuracy: 0.9084 - val_loss: 0.3031 - val_accuracy: 0.9005 - 1892s/epoch - 4s/step\n",
      "Epoch 10/15\n",
      "482/482 - 1993s - loss: 0.2705 - accuracy: 0.9097 - val_loss: 0.2967 - val_accuracy: 0.9035 - 1993s/epoch - 4s/step\n",
      "Epoch 11/15\n",
      "482/482 - 1352s - loss: 0.2648 - accuracy: 0.9121 - val_loss: 0.2919 - val_accuracy: 0.9057 - 1352s/epoch - 3s/step\n",
      "Epoch 12/15\n",
      "482/482 - 1438s - loss: 0.2591 - accuracy: 0.9131 - val_loss: 0.2934 - val_accuracy: 0.9077 - 1438s/epoch - 3s/step\n",
      "Epoch 13/15\n",
      "482/482 - 1276s - loss: 0.2560 - accuracy: 0.9145 - val_loss: 0.2899 - val_accuracy: 0.9079 - 1276s/epoch - 3s/step\n",
      "Epoch 14/15\n",
      "482/482 - 1137s - loss: 0.2503 - accuracy: 0.9158 - val_loss: 0.2907 - val_accuracy: 0.9082 - 1137s/epoch - 2s/step\n",
      "Epoch 15/15\n",
      "482/482 - 1181s - loss: 0.2444 - accuracy: 0.9177 - val_loss: 0.2912 - val_accuracy: 0.9081 - 1181s/epoch - 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcd4a661f10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_RCNN = Build_Model_RCNN_Text(word_index,embeddings_index, 5)\n",
    "\n",
    "\n",
    "model_RCNN.summary()\n",
    "\n",
    "model_RCNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=15,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_topic0=['violence', 'organization', 'government', 'practice', 'objective', 'social', 'religious', 'state', 'hundred', 'unlawful']\n",
    "term_topic1=['terrorist', 'militia', 'people', 'yemeni', 'houthi', 'year', 'eman', 'girl', 'village', 'support']\n",
    "term_topic3=['mine', 'international', 'home', 'plant', 'city', 'displace', 'terror', 'blow', 'threat', 'face']\n",
    "term_topic4=['group', 'crime', 'life', 'take', 'racist', 'believe', 'call', 'control', 'send', 'carry']\n",
    "term_topic5=['child', 'kill', 'recruit', 'political', 'houthis', 'civilian', 'war', 'woman', 'force', 'taiz']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic0=\"Religious organization violence\"\n",
    "Topic1=\"Houthi militias terrorize Yemeni people\"\n",
    "Topic3=\"international peace threats\"\n",
    "Topic4=\"Racist Tendencies\"\n",
    "Topic5=\"Recruitment and killing of children\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482/482 [==============================] - 108s 216ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted = model_RCNN.predict(X_test_Glove)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         precision    recall  f1-score   support\n",
      "\n",
      "        Religious organization violence       0.98      0.73      0.84      1296\n",
      "Houthi militias terrorize Yemeni people       0.87      0.99      0.93      8402\n",
      "            international peace threats       0.93      0.82      0.87      1138\n",
      "                      Racist Tendencies       0.97      0.85      0.91      1928\n",
      "    Recruitment and killing of children       0.96      0.82      0.89      2649\n",
      "\n",
      "                               accuracy                           0.91     15413\n",
      "                              macro avg       0.94      0.84      0.89     15413\n",
      "                           weighted avg       0.91      0.91      0.91     15413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = [Topic0, Topic1, Topic3, Topic4, Topic5]\n",
    "report=classification_report(y_test, predicted, target_names=target_names, digits=2)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
