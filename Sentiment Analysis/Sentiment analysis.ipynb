{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    phrase = re.sub(r\"isn\\'t\", \"is not\", phrase)\n",
    "    phrase = re.sub(r\"aren\\'t\", \"are not\", phrase)\n",
    "    phrase = re.sub(r\"shouldn\\'t\", \"should not\", phrase)\n",
    "    \n",
    "    phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n",
    "    phrase = re.sub(r\"hadn\\'t\", \"had not\", phrase)\n",
    "    phrase = re.sub(r\"don\\'t\", \"do not\", phrase)\n",
    "    \n",
    "    phrase = re.sub(r\"wouldn\\'t\", \"would not\", phrase)\n",
    "    phrase = re.sub(r\"doesn\\'t\", \"does not\", phrase)\n",
    "    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import treebank\n",
    "import pandas as pd\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words.remove('not')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "# Define function to lemmatize each word with its POS tag\n",
    " \n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "#sentiment_analyzer_scores(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   list_3=[]\\n    for i in pos_tagged:\\n            x=i[0],pos_tagger(i[1])\\n            list_3.append(x)\\n    Finally=[]\\n    for i in list_2:\\n        Finally.append(i[0])\\n    '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment_doc1(sentence):\n",
    "    sentence=decontracted(sentence)\n",
    "    list_1=[]\n",
    "    token=word_tokenize(sentence)\n",
    "    for term in token:\n",
    "        if term not in stop_words:\n",
    "            list_1.append(term)\n",
    "\n",
    "    # tokenize the sentence and find the POS tag for each token\n",
    "    pos_tagged = pos_tag(list_1) \n",
    "    #print(pos_tagged)\n",
    "    list_2=[]\n",
    "    for term_pos in pos_tagged:\n",
    "        if pos_tagger(term_pos[1]) =='a' or pos_tagger(term_pos[1]) =='r':\n",
    "            x=term_pos[0],pos_tagger(term_pos[1])\n",
    "            list_2.append(x)\n",
    "    Sent=' '.join(word for word in list_2)\n",
    "    return Sent\n",
    "'''\n",
    "   list_3=[]\n",
    "    for i in pos_tagged:\n",
    "            x=i[0],pos_tagger(i[1])\n",
    "            list_3.append(x)\n",
    "    Finally=[]\n",
    "    for i in list_2:\n",
    "        Finally.append(i[0])\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_doc(sentence):\n",
    "    sentence=decontracted(sentence)\n",
    "    list_1=[]\n",
    "    token=word_tokenize(sentence)\n",
    "    for term in token:\n",
    "        if term not in stop_words:\n",
    "            list_1.append(term)\n",
    "\n",
    "    # tokenize the sentence and find the POS tag for each token\n",
    "    pos_tagged = pos_tag(list_1) \n",
    "    #print(pos_tagged)\n",
    "    list_2=[]\n",
    "    for term_pos in pos_tagged:\n",
    "        list_2.append(term_pos[0])\n",
    "    Sent=' '.join(word for word in list_2)\n",
    "    return Sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=\"chant: death america plead new administration raise classification terrorists. steal food mouths hungry tell world starvation yemen. kill yemenis blow homes recruiy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chant : death america plead new administration raise classification terrorists . steal food mouths hungry tell world starvation yemen . kill yemenis blow homes recruiy ?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_doc(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.469, 'neu': 0.531, 'pos': 0.0, 'compound': -0.9509}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = analyser.polarity_scores(sent)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('../Violence in Yemens Social Media.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "positivity=\"positivity\"\n",
    "Negativity=\"Negativity\"\n",
    "lis=[]\n",
    "for doc_sent in data['Text']:\n",
    "    doc_sentiment=sentiment_doc(doc_sent)\n",
    "    score=analyser.polarity_scores(doc_sentiment)\n",
    "    if score['neg']> score['pos']:\n",
    "        lis.append(Negativity)\n",
    "    else:\n",
    "        lis.append(positivity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={'Polarity_Scores':lis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment_Data_classification=pd.DataFrame(dic)\n",
    "\n",
    "Sentiment_Data_classification.to_csv('Sentiment_Data_classification.csv',index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=pd.read_csv('Sentiment_Data_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "polarity_scores=data2.Polarity_Scores\n",
    "Text=data['Text']\n",
    "Dominant_Topic=data['Dominant_Topic']\n",
    "finally_Data={'Text':Text, 'Dominant_Topic':Dominant_Topic,'Polarity_Scores':polarity_scores}\n",
    "\n",
    "F_Data=pd.DataFrame(finally_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_Data.to_csv(\"Sentiment_Data_classification.csv\",index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Sentiment_Data_classification.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-50-ebde6634fcca>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-50-ebde6634fcca>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for term in data.Polarity_Scores:\n",
    "    if term==0:\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('Sentiment_Data_classification.xlsx',index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
